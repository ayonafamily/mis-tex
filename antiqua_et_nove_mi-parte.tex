\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{parskip}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{xurl}
\titleformat{\section}[block]{\Large\bfseries}{}{0em}{}
\pagestyle{fancy}
\fancyhf{}
\rhead{Exposición grupal}
\lhead{Antiqua et Nova}
\cfoot{\thepage}

\title{Análisis de ``Antiqua et Nova''\\ Inteligencia artificial y ética}
\author{Jorge L. Ayona Inglis}
\date{\today}

\begin{document}

\maketitle

\section*{Resumen de los núms. 100 y 101}

Los núms. 100 y 101 de \textit{Antiqua et Nova} abordan los riesgos del uso militar de la inteligencia artificial. El n. 100 denuncia el desarrollo de armas autónomas capaces de seleccionar y atacar objetivos sin intervención humana, lo cual plantea serias cuestiones éticas y jurídicas, especialmente en cuanto a la responsabilidad moral de sus acciones. El documento recuerda que la guerra no puede ser tratada como un simple cálculo técnico y urge a preservar la capacidad humana de decisión moral en situaciones de conflicto. El n. 101, por su parte, señala que la brecha entre estas armas y otras capaces de destrucción masiva es muy pequeña, por lo que algunos investigadores consideran que la IA representa un riesgo existencial. Ante esta amenaza, se reclama una reflexión seria sobre las implicancias de esta tecnología, siguiendo el llamado del Concilio Vaticano II a examinar la guerra con una mentalidad totalmente nueva.

\section*{Modelo de Toulmin del n. 100}

\begin{itemize}[leftmargin=2em]
  \item \textbf{Afirmación (Claim):} El uso de armas autónomas que pueden tomar decisiones letales sin intervención humana plantea serios problemas éticos y debe ser cuestionado.
  \item \textbf{Fundamento (Grounds):} Estas armas eluden la responsabilidad moral humana, ya que actúan sin que una persona tome la decisión directa de matar.
  \item \textbf{Garantía (Warrant):} Toda acción que implique quitar la vida debe estar sujeta a juicio moral humano.
  \item \textbf{Respaldo (Backing):} La enseñanza social de la Iglesia y el derecho internacional exigen responsabilidad moral en decisiones de vida o muerte.
  \item \textbf{Reserva (Rebuttal):} Algunos argumentan que estas armas reducen errores humanos, pero no justifican eliminar el juicio ético.
  \item \textbf{Calificador modal (Qualifier):} Es altamente cuestionable, desde una perspectiva ética y jurídica, permitir que algoritmos tomen decisiones irreversibles sobre vidas humanas sin supervisión responsable.
\end{itemize}

\section*{Modalizador}

El modalizador en el modelo de Toulmin expresa el grado de fuerza o certeza con que se sostiene una afirmación. En este caso, el modalizador es:

\begin{quote}
\textit{Es altamente cuestionable, desde una perspectiva ética y jurídica, permitir que algoritmos tomen decisiones irreversibles sobre vidas humanas sin supervisión responsable.}
\end{quote}

Este enunciado indica que la afirmación no se presenta como una verdad absoluta, pero sí como una postura con fuerte sustento ético y justificación racional. El uso del adverbio \textit{altamente} y la referencia explícita a criterios \textit{éticos y jurídicos} refuerzan el grado de compromiso con la tesis. Así, el modalizador cumple la función de precisar el nivel de certeza del argumento e invita a una reflexión seria sobre los límites morales del uso de armas autónomas.

\section*{Conclusión}

En conclusión, el uso de armas autónomas plantea una amenaza grave y creciente a los principios éticos fundamentales, ya que elimina la responsabilidad moral directa del ser humano en decisiones que implican la vida o la muerte. Aunque algunos defiendan estas tecnologías por su potencial para reducir errores humanos o aumentar la precisión en combate, este argumento resulta insuficiente frente a la gravedad de delegar decisiones letales a sistemas que carecen de conciencia, empatía y juicio moral.

Es altamente cuestionable, desde una perspectiva ética y jurídica, permitir que algoritmos tomen decisiones irreversibles sobre vidas humanas sin supervisión responsable. El grado de riesgo y deshumanización involucrado en estos sistemas hace que su uso sea, en la mayoría de los casos, moralmente inaceptable y contrario a la dignidad humana. Preservar la capacidad de discernimiento humano, especialmente en contextos de conflicto, no es solo deseable, sino imprescindible. En consecuencia, la comunidad internacional debería actuar con firmeza para establecer límites éticos y legales claros al desarrollo y uso de armamento autónomo, promoviendo una cultura de paz centrada en el respeto irreductible por cada vida humana.

\section*{Caso ilustrativo: RoboCop (1987)}

Una escena significativa de la película \textit{RoboCop} (1987) muestra al robot ED-209, diseñado para hacer cumplir la ley, matando a un ejecutivo en una presentación debido a un fallo en su sistema de reconocimiento de amenazas. A pesar de que el hombre dejó el arma, la máquina continuó la cuenta regresiva y disparó. Esto ilustra perfectamente los peligros del uso de IA autónoma sin control humano: falta de discernimiento, imposibilidad de detener el sistema, y consecuencias letales.

Tal como advierte \textit{Antiqua et Nova}, si la inteligencia artificial se usa al servicio de estructuras de poder sin control ético, se convierte en una amenaza real. La escena de \textit{RoboCop} se vuelve un ejemplo profético de los riesgos actuales cuando la IA es usada con fines autoritarios o militares, y ayuda a visualizar lo que el documento denomina \textbf{“riesgo existencial”}.

\newpage
%\section*{Escena}
\begin{quote}
\textbf{Video:} \href{https://www.youtube.com/watch?v=TYsulVXpgYg}{RoboCop (1987) -- ED-209 malfunction scene}
\end{quote}
\begin{figure}
\includegraphics[width=\linewidth, height=0.8\textheight, keepaspectratio]{robocop.png}
\end{figure}


\end{document}

